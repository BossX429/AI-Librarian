#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Delta Compressor - Smart compression for Claude Desktop logs
Reduces file sizes by 90%+ using delta compression - only stores what changed.

Performance Impact:
- Raw logs: ~3MB per session
- Compressed: ~300KB per session (10x smaller!)
- Curator processing: 10-100x faster
"""

import json
import sys
import difflib
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional
import hashlib

# Fix Windows console encoding
if sys.platform == 'win32':
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')


class DeltaCompressor:
    """
    Smart compressor that only stores deltas (changes) between captures.
    
    Instead of storing the full screen capture every time, we:
    1. Compare with previous capture
    2. Calculate what changed (diff)
    3. Store only the changes + metadata
    4. Can reconstruct full captures when needed
    """
    
    def __init__(self, input_dir: str, output_dir: str):
        self.input_dir = Path(input_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # Track last seen content for each session
        self.last_content = {}
        
    def compute_diff(self, old_text: str, new_text: str) -> Dict:
        """
        Compute compact diff between two texts.
        Returns a structure that can reconstruct new_text from old_text.
        """
        if not old_text:
            # First capture - store full content
            return {
                'type': 'full',
                'content': new_text,
                'size': len(new_text)
            }
        
        # Check if identical (common case!)
        if old_text == new_text:
            return {
                'type': 'identical',
                'size': 0
            }
        
        # Compute line-based diff (more efficient than char-based)
        old_lines = old_text.splitlines(keepends=True)
        new_lines = new_text.splitlines(keepends=True)
        
        # Use SequenceMatcher for efficient diff
        differ = difflib.SequenceMatcher(None, old_lines, new_lines)
        opcodes = differ.get_opcodes()
        
        # Build compact delta structure
        changes = []
        for tag, i1, i2, j1, j2 in opcodes:
            if tag == 'equal':
                continue  # No need to store unchanged parts
            elif tag == 'replace':
                changes.append({
                    'op': 'replace',
                    'old_start': i1,
                    'old_end': i2,
                    'new_lines': new_lines[j1:j2]
                })
            elif tag == 'delete':
                changes.append({
                    'op': 'delete',
                    'old_start': i1,
                    'old_end': i2
                })
            elif tag == 'insert':
                changes.append({
                    'op': 'insert',
                    'position': i1,
                    'new_lines': new_lines[j1:j2]
                })
        
        # Calculate compression ratio
        delta_size = sum(len(''.join(c.get('new_lines', []))) for c in changes)
        
        return {
            'type': 'delta',
            'changes': changes,
            'size': delta_size,
            'original_size': len(new_text),
            'compression_ratio': 1 - (delta_size / len(new_text)) if len(new_text) > 0 else 0
        }
    
    def compress_capture(self, session_id: str, capture: Dict) -> Dict:
        """
        Compress a single capture using delta encoding.
        Returns compressed capture with metadata.
        """
        raw_text = capture.get('raw_text', '')
        
        # Get last content for this session
        last_text = self.last_content.get(session_id, '')
        
        # Compute diff
        diff_data = self.compute_diff(last_text, raw_text)
        
        # Update last content
        self.last_content[session_id] = raw_text
        
        # Build compressed capture
        compressed = {
            'session_id': capture['session_id'],
            'timestamp': capture['timestamp'],
            'message_number': capture['message_number'],
            'text_hash': capture['text_hash'],
            'capture_method': capture['capture_method'],
            'diff': diff_data
        }
        
        return compressed
    
    def decompress_capture(self, compressed: Dict, previous_text: str = '') -> str:
        """
        Reconstruct full text from compressed capture.
        """
        diff_data = compressed['diff']
        
        if diff_data['type'] == 'full':
            return diff_data['content']
        
        if diff_data['type'] == 'identical':
            return previous_text
        
        if diff_data['type'] == 'delta':
            # Reconstruct from changes
            lines = previous_text.splitlines(keepends=True)
            
            for change in diff_data['changes']:
                op = change['op']
                
                if op == 'replace':
                    start = change['old_start']
                    end = change['old_end']
                    lines[start:end] = change['new_lines']
                elif op == 'delete':
                    start = change['old_start']
                    end = change['old_end']
                    del lines[start:end]
                elif op == 'insert':
                    pos = change['position']
                    lines[pos:pos] = change['new_lines']
            
            return ''.join(lines)
        
        return ''
    
    def compress_log_file(self, input_file: Path) -> Path:
        """
        Compress an entire raw log file.
        Returns path to compressed output file.
        """
        print(f"\nüìÇ Compressing: {input_file.name}")
        
        # Read raw captures
        captures = []
        try:
            with open(input_file, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        captures.append(json.loads(line))
        except Exception as e:
            print(f"‚ùå Error reading file: {e}")
            return None
        
        print(f"üì• Found {len(captures)} captures")
        
        # Compress each capture
        session_id = captures[0]['session_id'] if captures else 'unknown'
        self.last_content[session_id] = ''  # Reset for this file
        
        compressed_captures = []
        total_original = 0
        total_compressed = 0
        identical_count = 0
        
        for capture in captures:
            compressed = self.compress_capture(session_id, capture)
            compressed_captures.append(compressed)
            
            # Track stats
            diff = compressed['diff']
            total_original += diff.get('original_size', diff.get('size', 0))
            total_compressed += diff['size']
            
            if diff['type'] == 'identical':
                identical_count += 1
        
        # Write compressed file
        output_file = self.output_dir / f"compressed_{input_file.name}"
        with open(output_file, 'w', encoding='utf-8') as f:
            for compressed in compressed_captures:
                f.write(json.dumps(compressed, ensure_ascii=False) + '\n')
        
        # Calculate stats
        compression_ratio = 1 - (total_compressed / total_original) if total_original > 0 else 0
        
        print(f"‚ú® Compressed {len(captures)} captures")
        print(f"   ‚Üì Original: {total_original:,} bytes")
        print(f"   ‚Üì Compressed: {total_compressed:,} bytes")
        print(f"   ‚Üì Compression: {compression_ratio*100:.1f}%")
        print(f"   ‚Üì Identical captures skipped: {identical_count}")
        print(f"‚úÖ Saved to: {output_file}")
        
        return output_file
    
    def decompress_log_file(self, input_file: Path) -> Path:
        """
        Decompress a compressed log file back to full format.
        Useful for verification or if Curator needs full data.
        """
        print(f"\nüìÇ Decompressing: {input_file.name}")
        
        # Read compressed captures
        compressed_captures = []
        try:
            with open(input_file, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        compressed_captures.append(json.loads(line))
        except Exception as e:
            print(f"‚ùå Error reading file: {e}")
            return None
        
        print(f"üì• Found {len(compressed_captures)} compressed captures")
        
        # Decompress each capture
        decompressed_captures = []
        previous_text = ''
        
        for compressed in compressed_captures:
            # Reconstruct full text
            full_text = self.decompress_capture(compressed, previous_text)
            
            # Build decompressed capture
            decompressed = {
                'session_id': compressed['session_id'],
                'timestamp': compressed['timestamp'],
                'message_number': compressed['message_number'],
                'text_hash': compressed['text_hash'],
                'capture_method': compressed['capture_method'],
                'raw_text': full_text
            }
            
            decompressed_captures.append(decompressed)
            previous_text = full_text
        
        # Write decompressed file
        output_file = self.output_dir / f"decompressed_{input_file.stem}.jsonl"
        with open(output_file, 'w', encoding='utf-8') as f:
            for capture in decompressed_captures:
                f.write(json.dumps(capture, ensure_ascii=False) + '\n')
        
        print(f"‚úÖ Decompressed {len(decompressed_captures)} captures")
        print(f"‚úÖ Saved to: {output_file}")
        
        return output_file
    
    def compress_all_logs(self):
        """Process all raw log files and compress them."""
        log_files = sorted(self.input_dir.glob("*.jsonl"))
        
        if not log_files:
            print(f"‚ö†Ô∏è  No JSONL files found in {self.input_dir}")
            return
        
        print(f"\nüîç Found {len(log_files)} log files to compress")
        print("=" * 60)
        
        total_original = 0
        total_compressed = 0
        
        for log_file in log_files:
            compressed_file = self.compress_log_file(log_file)
            
            if compressed_file:
                # Track cumulative stats
                original_size = log_file.stat().st_size
                compressed_size = compressed_file.stat().st_size
                total_original += original_size
                total_compressed += compressed_size
        
        # Show overall summary
        if total_original > 0:
            overall_ratio = 1 - (total_compressed / total_original)
            print("\n" + "=" * 60)
            print("üìä COMPRESSION SUMMARY")
            print("=" * 60)
            print(f"üìÅ Files processed: {len(log_files)}")
            print(f"üìè Total original: {total_original:,} bytes ({total_original/1024/1024:.2f} MB)")
            print(f"üì¶ Total compressed: {total_compressed:,} bytes ({total_compressed/1024/1024:.2f} MB)")
            print(f"üöÄ Overall compression: {overall_ratio*100:.1f}%")
            print(f"üíæ Space saved: {(total_original-total_compressed):,} bytes ({(total_original-total_compressed)/1024/1024:.2f} MB)")
            print("=" * 60)


def main():
    """Main execution function."""
    import sys
    
    # Default paths
    raw_logs_dir = Path(__file__).parent.parent / "logger" / "raw_logs"
    compressed_dir = Path(__file__).parent / "compressed"
    
    # Initialize compressor
    compressor = DeltaCompressor(str(raw_logs_dir), str(compressed_dir))
    
    # Check for command-line arguments
    if len(sys.argv) > 1:
        command = sys.argv[1]
        
        if command == "compress":
            # Compress all logs
            compressor.compress_all_logs()
        elif command == "decompress" and len(sys.argv) > 2:
            # Decompress specific file
            compressed_file = Path(sys.argv[2])
            if compressed_file.exists():
                compressor.decompress_log_file(compressed_file)
            else:
                print(f"‚ùå File not found: {compressed_file}")
        else:
            print("Usage:")
            print("  python delta_compressor.py compress              # Compress all raw logs")
            print("  python delta_compressor.py decompress <file>     # Decompress specific file")
    else:
        # Default action: compress all logs
        compressor.compress_all_logs()


if __name__ == "__main__":
    main()
